# -*- coding: utf-8 -*-
"""JR_with_JD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1luDmr_fLTRpMR0fzhnP7tOeDGw79Ry-T
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import spacy
import warnings
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import nltk

warnings.filterwarnings('ignore')


nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')


def load_and_preprocess_data(internships_path: str, students_path: str,
                             internship_sample_size: int = 500,
                             student_sample_size: int = 100) -> tuple:
    """
    Load and preprocess the internship and student data with sampling.
    """
    try:

        with open(internships_path, 'r') as file:
            internships_data = json.load(file)

            if len(internships_data) > internship_sample_size:
                internships_data = np.random.choice(
                    internships_data,
                    size=internship_sample_size,
                    replace=False
                ).tolist()
            internships = pd.DataFrame(internships_data)

        with open(students_path, 'r') as file:
            students_data = json.load(file)

            if len(students_data) > student_sample_size:
                students_data = np.random.choice(
                    students_data,
                    size=student_sample_size,
                    replace=False
                ).tolist()
            students = pd.DataFrame(students_data)


        internships['requirements'] = internships['requirements'].apply(
            lambda x: ' '.join(x) if isinstance(x, list) else '')
        students['skills'] = students['skills'].apply(
            lambda x: ' '.join(x) if isinstance(x, list) else '')

        print(f"Loaded {len(internships)} internships and {len(students)} student profiles")
        return internships, students

    except Exception as e:
        print(f"Error loading data: {str(e)}")
        raise


def extract_keywords_from_jd(jd: str) -> List[str]:
    """
    Extract relevant keywords from job description using NLP techniques.
    """
    # Load spaCy model
    nlp = spacy.load('en_core_web_sm')


    doc = nlp(jd.lower())


    keywords = []


    for chunk in doc.noun_chunks:
        if not chunk.text.strip() in stopwords.words('english'):
            keywords.append(chunk.text.strip())


    relevant_entities = ['SKILL', 'ORG', 'PRODUCT', 'GPE', 'LANGUAGE']
    for ent in doc.ents:
        if ent.label_ in relevant_entities:
            keywords.append(ent.text.strip())


    for token in doc:
        if token.pos_ == 'VERB' and token.text not in stopwords.words('english'):
            keywords.append(token.text)

    return list(set(keywords))


def create_combined_vectors(internships: pd.DataFrame,
                            students: pd.DataFrame,
                            weight_jd: float = 0.3) -> Tuple:
    """
    Create TF-IDF vectors combining requirements, job descriptions, and skills.
    """
    try:

        internships['jd_keywords'] = internships['jd'].apply(
            extract_keywords_from_jd)


        internships['combined_text'] = internships.apply(
            lambda x: f"{x['requirements']} {' '.join(x['jd_keywords'])}",
            axis=1)

        # Create and configure vectorizer
        vectorizer = TfidfVectorizer(
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_features=5000  # Limit features to prevent memory issues
        )

        # Create vectors with proper weighting
        internship_vectors = vectorizer.fit_transform(internships['combined_text'])

        # Apply JD weighting to the relevant features
        jd_mask = internship_vectors.copy()
        jd_mask.data *= weight_jd

        # Transform student skills
        student_vectors = vectorizer.transform(students['skills'])

        return vectorizer, internship_vectors, student_vectors

    except Exception as e:
        print(f"Error in vectorization: {str(e)}")
        raise


def calculate_skill_match_score(student_skills: List[str],
                                internship_reqs: List[str],
                                jd_keywords: List[str]) -> Dict:
    """
    Calculate detailed skill matching scores.
    """
    student_skills_set = set(student_skills)
    requirements_set = set(internship_reqs)
    jd_keywords_set = set(jd_keywords)


    direct_matches = student_skills_set.intersection(requirements_set)
    keyword_matches = student_skills_set.intersection(jd_keywords_set)

    return {
        'direct_match_score': len(direct_matches) / len(requirements_set) if requirements_set else 0,
        'keyword_match_score': len(keyword_matches) / len(jd_keywords_set) if jd_keywords_set else 0,
        'direct_matches': list(direct_matches),
        'keyword_matches': list(keyword_matches)
    }


def generate_recommendations(students: pd.DataFrame,
                             internships: pd.DataFrame,
                             student_vectors: object,
                             internship_vectors: object,
                             top_n: int = 3,
                             min_similarity: float = 0.1) -> Dict:
    """
    Generate personalized internship recommendations with enhanced matching.
    """
    recommendations = {}

    for idx, student in students.iterrows():
        # Calculate TF-IDF similarities
        similarity_scores = cosine_similarity(
            student_vectors[idx], internship_vectors).flatten()

        # Get top recommendations above threshold
        top_indices = (-similarity_scores).argsort()
        filtered_indices = [i for i in top_indices
                            if similarity_scores[i] >= min_similarity][:top_n]

        # Store recommendations with details
        student_recs = []
        for intern_idx in filtered_indices:
            internship = internships.iloc[intern_idx]

            # Calculate detailed skill matches
            skill_matches = calculate_skill_match_score(
                student['skills'].split(),
                internship['requirements'].split(),
                internship['jd_keywords']
            )

            # Calculate weighted final score
            final_score = (
                    similarity_scores[intern_idx] * 0.4 +
                    skill_matches['direct_match_score'] * 0.4 +
                    skill_matches['keyword_match_score'] * 0.2
            )

            student_recs.append({
                'title': internship['title'],
                'company': internship['company'],
                'location': internship['location'],
                'similarity_score': round(final_score, 3),
                'tfidf_similarity': round(similarity_scores[intern_idx], 3),
                'direct_match_score': round(skill_matches['direct_match_score'], 3),
                'keyword_match_score': round(skill_matches['keyword_match_score'], 3),
                'direct_matches': skill_matches['direct_matches'],
                'keyword_matches': skill_matches['keyword_matches'],
                'jd_keywords': internship['jd_keywords'],
                'requirements': internship['requirements'].split()
            })

        # Sort recommendations by final score
        student_recs.sort(key=lambda x: x['similarity_score'], reverse=True)
        recommendations[student['name']] = student_recs

    return recommendations


def visualize_recommendations(recommendations: Dict):
    """
    Create enhanced visualizations for recommendation results.
    """
    # Create subplots for different metrics
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))

    # Prepare data for visualization
    scores_data = []
    for student, recs in recommendations.items():
        for rec in recs:
            scores_data.append({
                'Student': student,
                'Internship': f"{rec['title']} at {rec['company']}",
                'Overall Score': rec['similarity_score'],
                'TF-IDF Score': rec['tfidf_similarity'],
                'Direct Match': rec['direct_match_score'],
                'Keyword Match': rec['keyword_match_score']
            })

    scores_df = pd.DataFrame(scores_data)

    # Overall similarity heatmap
    pivot_table = scores_df.pivot(index='Student',
                                  columns='Internship',
                                  values='Overall Score')
    sns.heatmap(pivot_table, annot=True, cmap='YlOrRd', fmt='.3f', ax=ax1)
    ax1.set_title('Overall Recommendation Scores')
    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')

    # Component scores comparison
    scores_df.melt(id_vars=['Student', 'Internship'],
                   value_vars=['TF-IDF Score', 'Direct Match', 'Keyword Match'],
                   var_name='Score Type',
                   value_name='Score').boxplot(by='Score Type', ax=ax2)
    ax2.set_title('Distribution of Different Score Components')

    plt.tight_layout()
    plt.show()


def print_recommendations(recommendations: Dict):
    """
    Print detailed formatted recommendations for each student.
    """
    for student, recs in recommendations.items():
        print(f"\nRecommendations for {student}")
        print("=" * 50)

        for i, rec in enumerate(recs, 1):
            print(f"\n{i}. {rec['title']}")
            print(f"   Company: {rec['company']}")
            print(f"   Location: {rec['location']}")
            print(f"   Overall Match Score: {rec['similarity_score']}")
            print(f"   Score Breakdown:")
            print(f"   - TF-IDF Similarity: {rec['tfidf_similarity']}")
            print(f"   - Direct Skill Match: {rec['direct_match_score']}")
            print(f"   - JD Keyword Match: {rec['keyword_match_score']}")
            print(f"   Matching Skills: {', '.join(rec['direct_matches'])}")
            print(f"   Matching Keywords: {', '.join(rec['keyword_matches'])}")
            print(f"   All Requirements: {', '.join(rec['requirements'])}")
            print(f"   Key JD Terms: {', '.join(rec['jd_keywords'])}")


def save_recommendations(recommendations: Dict, output_path: str):
    """
    Save recommendations to a JSON file.

    Args:
        recommendations (Dict): Dictionary containing recommendations
        output_path (str): Path where to save the JSON file
    """
    try:
        # Convert any non-serializable objects to strings/basic types
        serializable_recommendations = {}
        for student, recs in recommendations.items():
            serializable_recommendations[student] = []
            for rec in recs:
                # Create a copy of the recommendation dict
                serializable_rec = rec.copy()
                # Convert any sets to lists
                serializable_rec['direct_matches'] = list(rec['direct_matches'])
                serializable_rec['keyword_matches'] = list(rec['keyword_matches'])
                serializable_rec['jd_keywords'] = list(rec['jd_keywords'])
                serializable_recommendations[student].append(serializable_rec)

        # Save to JSON file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_recommendations, f, indent=2, ensure_ascii=False)

        print(f"Recommendations successfully saved to {output_path}")

    except Exception as e:
        print(f"Error saving recommendations: {str(e)}")
        raise


def main():
    """
    Main function to run the enhanced recommendation system.
    """
    try:
        # Load and preprocess data
        internships, students = load_and_preprocess_data(
            '/Users/abhishekkanade/Desktop/ft-01-main/internship_data.json',
            '/Users/abhishekkanade/Desktop/ft-01-main/student_data.json',
            internship_sample_size=500,  # Sample 500 internships
            student_sample_size=100  # Sample 100 students
        )

        # Create vectors with combined text
        vectorizer, internship_vectors, student_vectors = create_combined_vectors(
            internships, students)

        # Generate recommendations
        recommendations = generate_recommendations(
            students, internships, student_vectors, internship_vectors)

        # Visualize results
        # visualize_recommendations(recommendations)

        # Print detailed recommendations
        print_recommendations(recommendations)

        # Save results
        save_recommendations(recommendations, '/content/recommendations.json')

        return recommendations

    except Exception as e:
        print(f"Error in recommendation system: {str(e)}")
        raise


if __name__ == "__main__":
    recommendations = main()
